{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2037c23882243a98ad9d03714e17ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38a35d8715a34165ace8633cecf544ea",
              "IPY_MODEL_f5079a75ff234f44bf62ad9627fbde3d",
              "IPY_MODEL_f0180eb54495485991263d7498ae9f5f"
            ],
            "layout": "IPY_MODEL_4e785e5c0e614a54bcab5a522c5a8985"
          }
        },
        "38a35d8715a34165ace8633cecf544ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e6953bf3d6540c1b60597f06dd6992a",
            "placeholder": "​",
            "style": "IPY_MODEL_91f0471bbc7145a6bbb88a0a7528e266",
            "value": "1_Pooling%2Fconfig.json: 100%"
          }
        },
        "f5079a75ff234f44bf62ad9627fbde3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f331fa396d7342eb9d974e300183a813",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1820edcac18440779250ae0f2bc0f64b",
            "value": 190
          }
        },
        "f0180eb54495485991263d7498ae9f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f91032c436447dbb4ab30246818cc94",
            "placeholder": "​",
            "style": "IPY_MODEL_b9d1388a59144e8c9cad9f98e22edcee",
            "value": " 190/190 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "4e785e5c0e614a54bcab5a522c5a8985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6953bf3d6540c1b60597f06dd6992a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f0471bbc7145a6bbb88a0a7528e266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f331fa396d7342eb9d974e300183a813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1820edcac18440779250ae0f2bc0f64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f91032c436447dbb4ab30246818cc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9d1388a59144e8c9cad9f98e22edcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter Codebase RAG Project"
      ],
      "metadata": {
        "id": "FTmVgAC90r3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-11-25 at 7 12 58 PM](https://github.com/user-attachments/assets/48dd9de1-b4d2-4318-8f52-85ec209d8ebc)"
      ],
      "metadata": {
        "id": "JSQbb-WI0Nb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libraries"
      ],
      "metadata": {
        "id": "MpmkP4rM1KRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pygithub langchain langchain-community openai tiktoken pinecone-client langchain_pinecone sentence-transformers"
      ],
      "metadata": {
        "id": "BGFWnzpBDkWH",
        "outputId": "e860b953-b67f-43c2-8aae-d18949126450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygithub\n",
            "  Downloading PyGithub-2.6.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting pynacl>=1.4.0 (from pygithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (2.3.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pygithub) (1.2.18)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.8.2)\n",
            "Collecting pinecone<6.0.0,>=5.4.0 (from langchain_pinecone)\n",
            "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.3.4)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.25.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.8.2-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone<6.0.0,>=5.4.0->langchain_pinecone)\n",
            "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->pygithub) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->pygithub) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pygithub) (1.17.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.5.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading PyGithub-2.6.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading langchain_pinecone-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_tests-0.3.12-py3-none-any.whl (37 kB)\n",
            "Downloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.25.3-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading syrupy-4.8.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pinecone-plugin-interface, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, syrupy, pytest-socket, pytest-asyncio, pynacl, pinecone-plugin-inference, pinecone-client, nvidia-cusparse-cu12, nvidia-cudnn-cu12, aiohttp, pydantic-settings, pinecone, nvidia-cusolver-cu12, dataclasses-json, pygithub, langchain-tests, langchain_pinecone, langchain-community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.13\n",
            "    Uninstalling aiohttp-3.11.13:\n",
            "      Successfully uninstalled aiohttp-3.11.13\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiohttp-3.10.11 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.18 langchain-tests-0.3.12 langchain_pinecone-0.2.3 marshmallow-3.26.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pinecone-5.4.2 pinecone-client-6.0.0 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7 pydantic-settings-2.8.1 pygithub-2.6.1 pynacl-1.5.0 pytest-asyncio-0.25.3 pytest-socket-0.7.0 python-dotenv-1.0.1 syrupy-4.8.2 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAIXpUxWDFSV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "import tempfile\n",
        "from github import Github, Repository\n",
        "from git import Repo\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from langchain.schema import Document\n",
        "from pinecone import Pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone a GitHub Repo locally"
      ],
      "metadata": {
        "id": "hTLsQ9Ma1FpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "github_repo = 'https://github.com/Ayush-Chaudhary/BLIP'"
      ],
      "metadata": {
        "id": "cftPfN27lM4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repository(repo_url):\n",
        "    \"\"\"Clones a GitHub repository to a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        repo_url: The URL of the GitHub repository.\n",
        "\n",
        "    Returns:\n",
        "        The path to the cloned repository.\n",
        "    \"\"\"\n",
        "    repo_name = github_repo.split('/')[-1]\n",
        "    repo_path = f\"/content/{repo_name}\"\n",
        "    Repo.clone_from(repo_url, str(repo_path))\n",
        "    return repo_path\n"
      ],
      "metadata": {
        "id": "F_1zslPsDmJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = clone_repository(github_repo)"
      ],
      "metadata": {
        "id": "hFrrr5rjEfYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "github_repo"
      ],
      "metadata": {
        "id": "SOIPXbV_KvIT",
        "outputId": "d9d246ab-24d8-4893-fe4a-dd525aaf0cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://github.com/Ayush-Chaudhary/BLIP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define which types of files to parse and which files / folders to ignore"
      ],
      "metadata": {
        "id": "7rm1vwr5KVQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',\n",
        "                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h', '.py'}\n",
        "\n",
        "IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',\n",
        "                '__pycache__', '.next', '.vscode', 'vendor'}"
      ],
      "metadata": {
        "id": "MQOcyi6DE5bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_content(file_path, repo_path):\n",
        "    \"\"\"\n",
        "    Get content of a single file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: Dictionary with file name and content\n",
        "    \"\"\"\n",
        "    try:\n",
        "      with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "      rel_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "      return {\n",
        "          \"name\": rel_path,\n",
        "          \"content\": content\n",
        "      }\n",
        "    except Exception as e:\n",
        "      print(f\"Error reading file {file_path}: {e}\")\n",
        "      return None\n",
        "\n",
        "\n",
        "def get_main_files_content(repo_path: str):\n",
        "    \"\"\"\n",
        "    Get content of supported code files from the local repository.\n",
        "\n",
        "    Args:\n",
        "        repo_path: Path to the local repository\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file names and contents\n",
        "    \"\"\"\n",
        "    files_content = []\n",
        "\n",
        "\n",
        "    try:\n",
        "      for root, _, files in os.walk(repo_path):\n",
        "        if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):\n",
        "          continue\n",
        "\n",
        "        for file in files:\n",
        "          file_path = os.path.join(root, file)\n",
        "          if os.path.splitext(file_path)[1] in SUPPORTED_EXTENSIONS:\n",
        "            file_content = get_file_content(file_path, repo_path)\n",
        "            if file_content:\n",
        "              files_content.append(file_content)\n",
        "    except Exception as e:\n",
        "      print(f\"Error reading files: {e}\")\n",
        "\n",
        "    return files_content"
      ],
      "metadata": {
        "id": "qi0FbfdrF6Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_content = get_main_files_content(path)"
      ],
      "metadata": {
        "id": "5mMaHXkrKoFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_content[0]"
      ],
      "metadata": {
        "id": "HvNX_WcUKoH6",
        "outputId": "948971a9-cfdc-4c03-afb6-377b9872f93e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'predict.py',\n",
              " 'content': '\"\"\"\\nDownload the weights in ./checkpoints beforehand for fast inference\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_vqa.pth\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\\n\"\"\"\\n\\nfrom pathlib import Path\\n\\nfrom PIL import Image\\nimport torch\\nfrom torchvision import transforms\\nfrom torchvision.transforms.functional import InterpolationMode\\nimport cog\\n\\nfrom models.blip import blip_decoder\\nfrom models.blip_vqa import blip_vqa\\nfrom models.blip_itm import blip_itm\\n\\n\\nclass Predictor(cog.Predictor):\\n    def setup(self):\\n        self.device = \"cuda:0\"\\n\\n        self.models = {\\n            \\'image_captioning\\': blip_decoder(pretrained=\\'checkpoints/model*_base_caption.pth\\',\\n                                             image_size=384, vit=\\'base\\'),\\n            \\'visual_question_answering\\': blip_vqa(pretrained=\\'checkpoints/model*_vqa.pth\\',\\n                                                  image_size=480, vit=\\'base\\'),\\n            \\'image_text_matching\\': blip_itm(pretrained=\\'checkpoints/model_base_retrieval_coco.pth\\',\\n                                            image_size=384, vit=\\'base\\')\\n        }\\n\\n    @cog.input(\\n        \"image\",\\n        type=Path,\\n        help=\"input image\",\\n    )\\n    @cog.input(\\n        \"task\",\\n        type=str,\\n        default=\\'image_captioning\\',\\n        options=[\\'image_captioning\\', \\'visual_question_answering\\', \\'image_text_matching\\'],\\n        help=\"Choose a task.\",\\n    )\\n    @cog.input(\\n        \"question\",\\n        type=str,\\n        default=None,\\n        help=\"Type question for the input image for visual question answering task.\",\\n    )\\n    @cog.input(\\n        \"caption\",\\n        type=str,\\n        default=None,\\n        help=\"Type caption for the input image for image text matching task.\",\\n    )\\n    def predict(self, image, task, question, caption):\\n        if task == \\'visual_question_answering\\':\\n            assert question is not None, \\'Please type a question for visual question answering task.\\'\\n        if task == \\'image_text_matching\\':\\n            assert caption is not None, \\'Please type a caption for mage text matching task.\\'\\n\\n        im = load_image(image, image_size=480 if task == \\'visual_question_answering\\' else 384, device=self.device)\\n        model = self.models[task]\\n        model.eval()\\n        model = model.to(self.device)\\n\\n        if task == \\'image_captioning\\':\\n            with torch.no_grad():\\n                caption = model.generate(im, sample=False, num_beams=3, max_length=20, min_length=5)\\n                return \\'Caption: \\' + caption[0]\\n\\n        if task == \\'visual_question_answering\\':\\n            with torch.no_grad():\\n                answer = model(im, question, train=False, inference=\\'generate\\')\\n                return \\'Answer: \\' + answer[0]\\n\\n        # image_text_matching\\n        itm_output = model(im, caption, match_head=\\'itm\\')\\n        itm_score = torch.nn.functional.softmax(itm_output, dim=1)[:, 1]\\n        itc_score = model(im, caption, match_head=\\'itc\\')\\n        return f\\'The image and text is matched with a probability of {itm_score.item():.4f}.\\\\n\\' \\\\\\n               f\\'The image feature and text feature has a cosine similarity of {itc_score.item():.4f}.\\'\\n\\n\\ndef load_image(image, image_size, device):\\n    raw_image = Image.open(str(image)).convert(\\'RGB\\')\\n\\n    w, h = raw_image.size\\n\\n    transform = transforms.Compose([\\n        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\\n    ])\\n    image = transform(raw_image).unsqueeze(0).to(device)\\n    return image\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "fTHEOUgp1Nmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ],
      "metadata": {
        "id": "pRz7UnvJoL-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'I like coding'\n",
        "embeddings = get_huggingface_embeddings(text)"
      ],
      "metadata": {
        "id": "oe-7UwHGvCno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e2037c23882243a98ad9d03714e17ec7",
            "38a35d8715a34165ace8633cecf544ea",
            "f5079a75ff234f44bf62ad9627fbde3d",
            "f0180eb54495485991263d7498ae9f5f",
            "4e785e5c0e614a54bcab5a522c5a8985",
            "9e6953bf3d6540c1b60597f06dd6992a",
            "91f0471bbc7145a6bbb88a0a7528e266",
            "f331fa396d7342eb9d974e300183a813",
            "1820edcac18440779250ae0f2bc0f64b",
            "9f91032c436447dbb4ab30246818cc94",
            "b9d1388a59144e8c9cad9f98e22edcee"
          ]
        },
        "outputId": "2de1e301-400a-4090-de32-2ba8346db898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2037c23882243a98ad9d03714e17ec7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Pinecone\n",
        "**1. Create an account on [Pinecone.io](https://app.pinecone.io/)**\n",
        "\n",
        "**2. Create a new index called \"codebase-rag\" and set the dimensions to 768. Leave the rest of the settings as they are.**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 58 50 PM](https://github.com/user-attachments/assets/f5fda046-4087-432a-a8c2-86e061005238)\n",
        "\n",
        "\n",
        "\n",
        "**3. Create an API Key for Pinecone**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 44 37 PM](https://github.com/user-attachments/assets/e7feacc6-2bd1-472a-82e5-659f65624a88)\n",
        "\n",
        "\n",
        "**4. Store your Pinecone API Key within Google Colab's secrets section, and then enable access to it (see the blue checkmark)**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 45 25 PM](https://github.com/user-attachments/assets/eaf73083-0b5f-4d17-9e0c-eab84f91b0bc)\n",
        "\n"
      ],
      "metadata": {
        "id": "umKbNfk3aBOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the PINECONE_API_KEY as an environment variable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"codebase-rag\")"
      ],
      "metadata": {
        "id": "y05YK2IjaGgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "OQN1SdEQbwDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee607c6-cc43-4bbe-ef74-856514c0f80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-5982ffb8f713>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n",
            "<ipython-input-15-5982ffb8f713>:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the codebase embeddings into Pinecone\n",
        "\n",
        "documents = []\n",
        "\n",
        "for file in files_content[:2]:\n",
        "    doc = Document(\n",
        "        page_content=f\"{file['name']}\\n{file['content']}\",\n",
        "        metadata={\"source\": file['name']}\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    index_name=\"codebase-rag\",\n",
        "    namespace=\"https://github.com/Ayush-Chaudhary/BLIP\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "tDAB_siIb93B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47d2df6-2135-4e13-d10d-042facaebcac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-88847094a728>:14: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding=HuggingFaceEmbeddings(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGuQiFQmd4HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform RAG\n",
        "\n",
        "1. Get your OpenRouter API Key [here](https://openrouter.ai/settings/keys)\n",
        "\n",
        "2. Paste your OpenRouter Key into your Google Colab secrets, and make sure to enable permissions for it\n",
        "\n",
        "![Image](https://github.com/user-attachments/assets/bd64c5aa-952e-4a1e-9ac0-01d8fe93aaa1)\n"
      ],
      "metadata": {
        "id": "e75xrBVCrRL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "K9DJQMc_nrsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Give an overview of the Deep learning model\""
      ],
      "metadata": {
        "id": "MqJtdpK_qNut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)"
      ],
      "metadata": {
        "id": "hxQHkgNSLEnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches = pinecone_index.query(\n",
        "    vector=raw_query_embedding.tolist(),\n",
        "    top_k=3,\n",
        "    include_metadata=True,\n",
        "    namespace = 'https://github.com/Ayush-Chaudhary/BLIP'\n",
        ")"
      ],
      "metadata": {
        "id": "qpsGsYUXLEpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = [item['metadata']['text'] for item in top_matches['matches']]"
      ],
      "metadata": {
        "id": "7FwqRviPqkHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "LDKAXsu1qkKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7a8a7a-8bce-42de-e97c-3f470afd634e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['predict.py\\n\"\"\"\\nDownload the weights in ./checkpoints beforehand for fast inference\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_vqa.pth\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\\n\"\"\"\\n\\nfrom pathlib import Path\\n\\nfrom PIL import Image\\nimport torch\\nfrom torchvision import transforms\\nfrom torchvision.transforms.functional import InterpolationMode\\nimport cog\\n\\nfrom models.blip import blip_decoder\\nfrom models.blip_vqa import blip_vqa\\nfrom models.blip_itm import blip_itm\\n\\n\\nclass Predictor(cog.Predictor):\\n    def setup(self):\\n        self.device = \"cuda:0\"\\n\\n        self.models = {\\n            \\'image_captioning\\': blip_decoder(pretrained=\\'checkpoints/model*_base_caption.pth\\',\\n                                             image_size=384, vit=\\'base\\'),\\n            \\'visual_question_answering\\': blip_vqa(pretrained=\\'checkpoints/model*_vqa.pth\\',\\n                                                  image_size=480, vit=\\'base\\'),\\n            \\'image_text_matching\\': blip_itm(pretrained=\\'checkpoints/model_base_retrieval_coco.pth\\',\\n                                            image_size=384, vit=\\'base\\')\\n        }\\n\\n    @cog.input(\\n        \"image\",\\n        type=Path,\\n        help=\"input image\",\\n    )\\n    @cog.input(\\n        \"task\",\\n        type=str,\\n        default=\\'image_captioning\\',\\n        options=[\\'image_captioning\\', \\'visual_question_answering\\', \\'image_text_matching\\'],\\n        help=\"Choose a task.\",\\n    )\\n    @cog.input(\\n        \"question\",\\n        type=str,\\n        default=None,\\n        help=\"Type question for the input image for visual question answering task.\",\\n    )\\n    @cog.input(\\n        \"caption\",\\n        type=str,\\n        default=None,\\n        help=\"Type caption for the input image for image text matching task.\",\\n    )\\n    def predict(self, image, task, question, caption):\\n        if task == \\'visual_question_answering\\':\\n            assert question is not None, \\'Please type a question for visual question answering task.\\'\\n        if task == \\'image_text_matching\\':\\n            assert caption is not None, \\'Please type a caption for mage text matching task.\\'\\n\\n        im = load_image(image, image_size=480 if task == \\'visual_question_answering\\' else 384, device=self.device)\\n        model = self.models[task]\\n        model.eval()\\n        model = model.to(self.device)\\n\\n        if task == \\'image_captioning\\':\\n            with torch.no_grad():\\n                caption = model.generate(im, sample=False, num_beams=3, max_length=20, min_length=5)\\n                return \\'Caption: \\' + caption[0]\\n\\n        if task == \\'visual_question_answering\\':\\n            with torch.no_grad():\\n                answer = model(im, question, train=False, inference=\\'generate\\')\\n                return \\'Answer: \\' + answer[0]\\n\\n        # image_text_matching\\n        itm_output = model(im, caption, match_head=\\'itm\\')\\n        itm_score = torch.nn.functional.softmax(itm_output, dim=1)[:, 1]\\n        itc_score = model(im, caption, match_head=\\'itc\\')\\n        return f\\'The image and text is matched with a probability of {itm_score.item():.4f}.\\\\n\\' \\\\\\n               f\\'The image feature and text feature has a cosine similarity of {itc_score.item():.4f}.\\'\\n\\n\\ndef load_image(image, image_size, device):\\n    raw_image = Image.open(str(image)).convert(\\'RGB\\')\\n\\n    w, h = raw_image.size\\n\\n    transform = transforms.Compose([\\n        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\\n    ])\\n    image = transform(raw_image).unsqueeze(0).to(device)\\n    return image\\n',\n",
              " 'predict.py\\n\"\"\"\\nDownload the weights in ./checkpoints beforehand for fast inference\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_vqa.pth\\nwget https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\\n\"\"\"\\n\\nfrom pathlib import Path\\n\\nfrom PIL import Image\\nimport torch\\nfrom torchvision import transforms\\nfrom torchvision.transforms.functional import InterpolationMode\\nimport cog\\n\\nfrom models.blip import blip_decoder\\nfrom models.blip_vqa import blip_vqa\\nfrom models.blip_itm import blip_itm\\n\\n\\nclass Predictor(cog.Predictor):\\n    def setup(self):\\n        self.device = \"cuda:0\"\\n\\n        self.models = {\\n            \\'image_captioning\\': blip_decoder(pretrained=\\'checkpoints/model*_base_caption.pth\\',\\n                                             image_size=384, vit=\\'base\\'),\\n            \\'visual_question_answering\\': blip_vqa(pretrained=\\'checkpoints/model*_vqa.pth\\',\\n                                                  image_size=480, vit=\\'base\\'),\\n            \\'image_text_matching\\': blip_itm(pretrained=\\'checkpoints/model_base_retrieval_coco.pth\\',\\n                                            image_size=384, vit=\\'base\\')\\n        }\\n\\n    @cog.input(\\n        \"image\",\\n        type=Path,\\n        help=\"input image\",\\n    )\\n    @cog.input(\\n        \"task\",\\n        type=str,\\n        default=\\'image_captioning\\',\\n        options=[\\'image_captioning\\', \\'visual_question_answering\\', \\'image_text_matching\\'],\\n        help=\"Choose a task.\",\\n    )\\n    @cog.input(\\n        \"question\",\\n        type=str,\\n        default=None,\\n        help=\"Type question for the input image for visual question answering task.\",\\n    )\\n    @cog.input(\\n        \"caption\",\\n        type=str,\\n        default=None,\\n        help=\"Type caption for the input image for image text matching task.\",\\n    )\\n    def predict(self, image, task, question, caption):\\n        if task == \\'visual_question_answering\\':\\n            assert question is not None, \\'Please type a question for visual question answering task.\\'\\n        if task == \\'image_text_matching\\':\\n            assert caption is not None, \\'Please type a caption for mage text matching task.\\'\\n\\n        im = load_image(image, image_size=480 if task == \\'visual_question_answering\\' else 384, device=self.device)\\n        model = self.models[task]\\n        model.eval()\\n        model = model.to(self.device)\\n\\n        if task == \\'image_captioning\\':\\n            with torch.no_grad():\\n                caption = model.generate(im, sample=False, num_beams=3, max_length=20, min_length=5)\\n                return \\'Caption: \\' + caption[0]\\n\\n        if task == \\'visual_question_answering\\':\\n            with torch.no_grad():\\n                answer = model(im, question, train=False, inference=\\'generate\\')\\n                return \\'Answer: \\' + answer[0]\\n\\n        # image_text_matching\\n        itm_output = model(im, caption, match_head=\\'itm\\')\\n        itm_score = torch.nn.functional.softmax(itm_output, dim=1)[:, 1]\\n        itc_score = model(im, caption, match_head=\\'itc\\')\\n        return f\\'The image and text is matched with a probability of {itm_score.item():.4f}.\\\\n\\' \\\\\\n               f\\'The image feature and text feature has a cosine similarity of {itc_score.item():.4f}.\\'\\n\\n\\ndef load_image(image, image_size, device):\\n    raw_image = Image.open(str(image)).convert(\\'RGB\\')\\n\\n    w, h = raw_image.size\\n\\n    transform = transforms.Compose([\\n        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\\n    ])\\n    image = transform(raw_image).unsqueeze(0).to(device)\\n    return image\\n',\n",
              " 'train_retrieval.py\\n\\'\\'\\'\\n * Copyright (c) 2022, salesforce.com, inc.\\n * All rights reserved.\\n * SPDX-License-Identifier: BSD-3-Clause\\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\\n * By Junnan Li\\n\\'\\'\\'\\nimport argparse\\nimport os\\nimport ruamel_yaml as yaml\\nimport numpy as np\\nimport random\\nimport time\\nimport datetime\\nimport json\\nfrom pathlib import Path\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.backends.cudnn as cudnn\\nimport torch.distributed as dist\\nfrom torch.utils.data import DataLoader\\n\\nfrom models.blip_retrieval import blip_retrieval\\nimport utils\\nfrom utils import cosine_lr_schedule\\nfrom data import create_dataset, create_sampler, create_loader\\n\\n\\ndef train(model, data_loader, optimizer, epoch, device, config):\\n    # train\\n    model.train()  \\n    \\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\\n    metric_logger.add_meter(\\'lr\\', utils.SmoothedValue(window_size=1, fmt=\\'{value:.6f}\\'))\\n    metric_logger.add_meter(\\'loss_itm\\', utils.SmoothedValue(window_size=1, fmt=\\'{value:.4f}\\'))\\n    metric_logger.add_meter(\\'loss_ita\\', utils.SmoothedValue(window_size=1, fmt=\\'{value:.4f}\\'))\\n    header = \\'Train Epoch: [{}]\\'.format(epoch)\\n    print_freq = 50\\n\\n    for i,(image, caption, idx) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\\n        image = image.to(device,non_blocking=True)   \\n        idx = idx.to(device,non_blocking=True)   \\n       \\n        if epoch>0:\\n            alpha = config[\\'alpha\\']\\n        else:\\n            alpha = config[\\'alpha\\']*min(1,i/len(data_loader))\\n\\n        loss_ita, loss_itm = model(image, caption, alpha=alpha, idx=idx)                  \\n        loss = loss_ita + loss_itm\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()    \\n        \\n        metric_logger.update(loss_itm=loss_itm.item())\\n        metric_logger.update(loss_ita=loss_ita.item())\\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\\n\\n    # gather the stats from all processes\\n    metric_logger.synchronize_between_processes()\\n    print(\"Averaged stats:\", metric_logger.global_avg())     \\n    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}  \\n\\n\\n@torch.no_grad()\\ndef evaluation(model, data_loader, device, config):\\n    # test\\n    model.eval() \\n    \\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\\n    header = \\'Evaluation:\\'    \\n    \\n    print(\\'Computing features for evaluation...\\')\\n    start_time = time.time()  \\n\\n    texts = data_loader.dataset.text   \\n    num_text = len(texts)\\n    text_bs = 256\\n    text_ids = []\\n    text_embeds = []  \\n    text_atts = []\\n    for i in range(0, num_text, text_bs):\\n        text = texts[i: min(num_text, i+text_bs)]\\n        text_input = model.tokenizer(text, padding=\\'max_length\\', truncation=True, max_length=35, return_tensors=\"pt\").to(device) \\n        text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode=\\'text\\')  \\n        text_embed = F.normalize(model.text_proj(text_output.last_hidden_state[:,0,:]))\\n        text_embeds.append(text_embed)   \\n        text_ids.append(text_input.input_ids)\\n        text_atts.append(text_input.attention_mask)\\n    \\n    text_embeds = torch.cat(text_embeds,dim=0)\\n    text_ids = torch.cat(text_ids,dim=0)\\n    text_atts = torch.cat(text_atts,dim=0)\\n    text_ids[:,0] = model.tokenizer.enc_token_id\\n    \\n    image_feats = []\\n    image_embeds = []\\n    for image, img_id in data_loader: \\n        image = image.to(device) \\n        image_feat = model.visual_encoder(image)   \\n        image_embed = model.vision_proj(image_feat[:,0,:])            \\n        image_embed = F.normalize(image_embed,dim=-1)      \\n        \\n        image_feats.append(image_feat.cpu())\\n        image_embeds.append(image_embed)\\n     \\n    image_feats = torch.cat(image_feats,dim=0)\\n    image_embeds = torch.cat(image_embeds,dim=0)\\n    \\n    sims_matrix = image_embeds @ text_embeds.t()\\n    score_matrix_i2t = torch.full((len(data_loader.dataset.image),len(texts)),-100.0).to(device)\\n    \\n    num_tasks = utils.get_world_size()\\n    rank = utils.get_rank() \\n    step = sims_matrix.size(0)//num_tasks + 1\\n    start = rank*step\\n    end = min(sims_matrix.size(0),start+step)\\n\\n    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \\n        topk_sim, topk_idx = sims.topk(k=config[\\'k_test\\'], dim=0)\\n\\n        encoder_output = image_feats[start+i].repeat(config[\\'k_test\\'],1,1).to(device)\\n        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\\n        output = model.text_encoder(text_ids[topk_idx], \\n                                    attention_mask = text_atts[topk_idx],\\n                                    encoder_hidden_states = encoder_output,\\n                                    encoder_attention_mask = encoder_att,                             \\n                                    return_dict = True,\\n                                   )\\n        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\\n        score_matrix_i2t[start+i,topk_idx] = score + topk_sim\\n        \\n    sims_matrix = sims_matrix.t()\\n    score_matrix_t2i = torch.full((len(texts),len(data_loader.dataset.image)),-100.0).to(device)\\n    \\n    step = sims_matrix.size(0)//num_tasks + 1\\n    start = rank*step\\n    end = min(sims_matrix.size(0),start+step)    \\n    \\n    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \\n        \\n        topk_sim, topk_idx = sims.topk(k=config[\\'k_test\\'], dim=0)\\n        encoder_output = image_feats[topk_idx].to(device)\\n        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\\n        output = model.text_encoder(text_ids[start+i].repeat(config[\\'k_test\\'],1), \\n                                    attention_mask = text_atts[start+i].repeat(config[\\'k_test\\'],1),\\n                                    encoder_hidden_states = encoder_output,\\n                                    encoder_attention_mask = encoder_att,                             \\n                                    return_dict = True,\\n                                   )\\n        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\\n        score_matrix_t2i[start+i,topk_idx] = score + topk_sim\\n\\n    if args.distributed:\\n        dist.barrier()   \\n        torch.distributed.all_reduce(score_matrix_i2t, op=torch.distributed.ReduceOp.SUM) \\n        torch.distributed.all_reduce(score_matrix_t2i, op=torch.distributed.ReduceOp.SUM)        \\n        \\n    total_time = time.time() - start_time\\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\\n    print(\\'Evaluation time {}\\'.format(total_time_str)) \\n\\n    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()\\n\\n\\n            \\n@torch.no_grad()\\ndef itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\\n    \\n    #Images->Text \\n    ranks = np.zeros(scores_i2t.shape[0])\\n    for index,score in enumerate(scores_i2t):\\n        inds = np.argsort(score)[::-1]\\n        # Score\\n        rank = 1e20\\n        for i in img2txt[index]:\\n            tmp = np.where(inds == i)[0][0]\\n            if tmp < rank:\\n                rank = tmp\\n        ranks[index] = rank\\n\\n    # Compute metrics\\n    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\\n    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\\n    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\\n  \\n    #Text->Images \\n    ranks = np.zeros(scores_t2i.shape[0])\\n    \\n    for index,score in enumerate(scores_t2i):\\n        inds = np.argsort(score)[::-1]\\n        ranks[index] = np.where(inds == txt2img[index])[0][0]\\n\\n    # Compute metrics\\n    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\\n    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\\n    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)        \\n\\n    tr_mean = (tr1 + tr5 + tr10) / 3\\n    ir_mean = (ir1 + ir5 + ir10) / 3\\n    r_mean = (tr_mean + ir_mean) / 2\\n\\n    eval_result =  {\\'txt_r1\\': tr1,\\n                    \\'txt_r5\\': tr5,\\n                    \\'txt_r10\\': tr10,\\n                    \\'txt_r_mean\\': tr_mean,\\n                    \\'img_r1\\': ir1,\\n                    \\'img_r5\\': ir5,\\n                    \\'img_r10\\': ir10,\\n                    \\'img_r_mean\\': ir_mean,\\n                    \\'r_mean\\': r_mean}\\n    return eval_result\\n\\n\\ndef main(args, config):\\n    utils.init_distributed_mode(args)    \\n    \\n    device = torch.device(args.device)\\n\\n    # fix the seed for reproducibility\\n    seed = args.seed + utils.get_rank()\\n    torch.manual_seed(seed)\\n    np.random.seed(seed)\\n    random.seed(seed)\\n    cudnn.benchmark = True\\n\\n    #### Dataset #### \\n    print(\"Creating retrieval dataset\")\\n    train_dataset, val_dataset, test_dataset = create_dataset(\\'retrieval_%s\\'%config[\\'dataset\\'], config)  \\n\\n    if args.distributed:\\n        num_tasks = utils.get_world_size()\\n        global_rank = utils.get_rank()            \\n        samplers = create_sampler([train_dataset], [True], num_tasks, global_rank) + [None, None]\\n    else:\\n        samplers = [None, None, None]\\n    \\n    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\\n                                                          batch_size=[config[\\'batch_size_train\\']]+[config[\\'batch_size_test\\']]*2,\\n                                                          num_workers=[4,4,4],\\n                                                          is_trains=[True, False, False], \\n                                                          collate_fns=[None,None,None])   \\n   \\n\\n    #### Model #### \\n    print(\"Creating model\")\\n    model = blip_retrieval(pretrained=config[\\'pretrained\\'], image_size=config[\\'image_size\\'], vit=config[\\'vit\\'], \\n                             vit_grad_ckpt=config[\\'vit_grad_ckpt\\'], vit_ckpt_layer=config[\\'vit_ckpt_layer\\'], \\n                             queue_size=config[\\'queue_size\\'], negative_all_rank=config[\\'negative_all_rank\\'])\\n\\n    model = model.to(device)   \\n    \\n    model_without_ddp = model\\n    if args.distributed:\\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\\n        model_without_ddp = model.module   \\n\\n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=config[\\'init_lr\\'], weight_decay=config[\\'weight_decay\\']) \\n    \\n    best = 0\\n    best_epoch = 0\\n\\n    print(\"Start training\")\\n    start_time = time.time()    \\n\\n    for epoch in range(0, config[\\'max_epoch\\']):    \\n        if not args.evaluate:        \\n            if args.distributed:\\n                train_loader.sampler.set_epoch(epoch)\\n                \\n            cosine_lr_schedule(optimizer, epoch, config[\\'max_epoch\\'], config[\\'init_lr\\'], config[\\'min_lr\\'])\\n            \\n            train_stats = train(model, train_loader, optimizer, epoch, device, config)  \\n            \\n        score_val_i2t, score_val_t2i, = evaluation(model_without_ddp, val_loader, device, config)\\n        score_test_i2t, score_test_t2i = evaluation(model_without_ddp, test_loader, device, config)\\n    \\n        if utils.is_main_process():  \\n      \\n            val_result = itm_eval(score_val_i2t, score_val_t2i, val_loader.dataset.txt2img, val_loader.dataset.img2txt)  \\n            print(val_result)\\n                                \\n            if val_result[\\'r_mean\\']>best:\\n                save_obj = {\\n                    \\'model\\': model_without_ddp.state_dict(),\\n                    \\'optimizer\\': optimizer.state_dict(),\\n                    \\'config\\': config,\\n                    \\'epoch\\': epoch,\\n                }\\n                torch.save(save_obj, os.path.join(args.output_dir, \\'checkpoint_best.pth\\'))  \\n                best = val_result[\\'r_mean\\']        \\n                best_epoch = epoch  \\n                \\n                test_result = itm_eval(score_test_i2t, score_test_t2i, test_loader.dataset.txt2img, test_loader.dataset.img2txt) \\n                print(test_result)\\n            \\n            if args.evaluate:                \\n                log_stats = {**{f\\'val_{k}\\': v for k, v in val_result.items()},\\n                             **{f\\'test_{k}\\': v for k, v in test_result.items()},                  \\n                            }\\n                with open(os.path.join(args.output_dir, \"evaluate.txt\"),\"a\") as f:\\n                    f.write(json.dumps(log_stats) + \"\\\\n\")     \\n            else:\\n                log_stats = {**{f\\'train_{k}\\': v for k, v in train_stats.items()},\\n                             **{f\\'val_{k}\\': v for k, v in val_result.items()},\\n                             **{f\\'test_{k}\\': v for k, v in test_result.items()},  \\n                             \\'epoch\\': epoch,\\n                             \\'best_epoch\\': best_epoch,\\n                            }\\n                with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\\n                    f.write(json.dumps(log_stats) + \"\\\\n\")   \\n                    \\n        if args.evaluate: \\n            break\\n\\n        dist.barrier()     \\n        torch.cuda.empty_cache()\\n\\n    total_time = time.time() - start_time\\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\\n    print(\\'Training time {}\\'.format(total_time_str)) \\n\\n    \\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()     \\n    parser.add_argument(\\'--config\\', default=\\'./configs/retrieval_flickr.yaml\\')\\n    parser.add_argument(\\'--output_dir\\', default=\\'output/Retrieval_flickr\\')        \\n    parser.add_argument(\\'--evaluate\\', action=\\'store_true\\')\\n    parser.add_argument(\\'--device\\', default=\\'cuda\\')\\n    parser.add_argument(\\'--seed\\', default=42, type=int)\\n    parser.add_argument(\\'--world_size\\', default=1, type=int, help=\\'number of distributed processes\\')    \\n    parser.add_argument(\\'--dist_url\\', default=\\'env://\\', help=\\'url used to set up distributed training\\')\\n    parser.add_argument(\\'--distributed\\', default=True, type=bool)\\n    args = parser.parse_args()\\n\\n    config = yaml.load(open(args.config, \\'r\\'), Loader=yaml.Loader)\\n\\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\\n        \\n    yaml.dump(config, open(os.path.join(args.output_dir, \\'config.yaml\\'), \\'w\\'))    \\n    \\n    main(args, config)']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_query = f\"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know,\n",
        "don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Helpful Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "13IfOcxwzm9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_query = ''' Answer the question i have based on the content provided. Consider all the given content'''\n",
        "\n",
        "llm_response = client.chat.completions.create(\n",
        "    model = 'deepseek/deepseek-r1:free',\n",
        "    messages=[\n",
        "        {'role': 'system', 'content': system_query},\n",
        "        {'role': 'user', 'content': augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "zJNAMzD61OHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IALeQr2N1uru",
        "outputId": "3ae61593-b0fb-4ffd-e71c-c764c3438345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided code implements the **BLIP (Bootstrapping Language-Image Pre-training)** model, designed for vision-language tasks. Here's a concise overview:\n",
            "\n",
            "### Key Components:\n",
            "1. **Tasks Supported**:\n",
            "   - **Image Captioning**: Generates textual descriptions for input images.\n",
            "   - **Visual Question Answering (VQA)**: Answers natural language questions about images.\n",
            "   - **Image-Text Matching**: Evaluates alignment between an image and a text caption.\n",
            "\n",
            "2. **Model Architecture**:\n",
            "   - **Vision Transformer (ViT)**: Encodes images into visual features.\n",
            "   - **Text Transformer**: Processes text inputs (captions/questions).\n",
            "   - **Task-Specific Heads**:\n",
            "     - **Decoder**: Generates captions (for image captioning).\n",
            "     - **VQA Head**: Produces answers via text generation.\n",
            "     - **ITM (Image-Text Matching) Head**: Predicts alignment probability between image-text pairs.\n",
            "     - **ITC (Image-Text Contrastive) Head**: Measures cosine similarity between image and text embeddings.\n",
            "\n",
            "3. **Training Objectives (Retrieval)**:\n",
            "   - **Image-Text Contrastive Loss (ITA)**: Aligns image and text featu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2eiONrF2hBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}